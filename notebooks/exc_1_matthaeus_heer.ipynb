{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# Building a Robot Judge - Assignment 1\n",
    "## Matth√§us Heer\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from textwrap import indent\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from context import robot_judge  # Now we can use all the handy robot_judge functionality :-)\n",
    "from robot_judge.io import ProblemSet1Io\n",
    "from robot_judge.nlp.language_models import spacy_nlp, stop_words\n",
    "from robot_judge.utils import indent as indent_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To\n",
    "\n",
    "1) In the repo root directory there is a folder called _data_.  \n",
    "2) Go there and create a folder called *assignment_1* (or whatever the DATA_DIR_NAME variable below is assigned to).   \n",
    "3) Place all cases files (*1936_X9VD8L.txt*, ...) in there.  \n",
    "4) Place the *case_reversed.csv* file in there.  \n",
    "5) Run the code below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLED_CASES = 10  # How many cases should be sampled to work on.\n",
    "DATA_DIR_NAME = 'assignment_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Compute sentece, words and letters count per document and plot vs year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot_judge.exploration.corpus_analysis import count_words_sents_letters\n",
    "from robot_judge.exploration.corpus_analysis import visualize_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = ProblemSet1Io(data_dir=DATA_DIR_NAME)\n",
    "sampled_cases = io.read_multiple_cases_files(n_samples=N_SAMPLED_CASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takse ~40min for 1000 cases\n",
    "%time labels, word_counts, sents_counts, letters_counts = count_words_sents_letters(sampled_cases)\n",
    "years = [ProblemSet1Io.get_year_from_case_title(label) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_counts(years, word_counts, sents_counts, letters_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Plot part-of-speech (POS) tagging frequency vs year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot_judge.exploration.corpus_analysis import get_pos_tags\n",
    "from robot_judge.exploration.corpus_analysis import aggregate_avg_pos_tags\n",
    "from robot_judge.exploration.corpus_analysis import visualize_avg_pos_vs_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time years, pos_tags = get_pos_tags(sampled_cases, ProblemSet1Io.get_year_from_case_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = aggregate_avg_pos_tags(years, pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_avg_pos_vs_year(pos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 \n",
    "## Corpus normalization / cleaning & trigram creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot_judge.nlp.ngrams import aggregate_clean_sentences, train_phrase_model, get_sents_from_sentence_dict\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from robot_judge.nlp.ngrams import print_label_sent_dict\n",
    "from robot_judge.nlp import spacy_doc\n",
    "from robot_judge.nlp.filter import token_is_punct_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case label keys, full text values\n",
    "N_SAMPLED_CASES_FOR_NGRAMS = 10\n",
    "test_corpus_dict = io.read_multiple_cases_files(N_SAMPLED_CASES_FOR_NGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case label keys, list (sentences) of lists (words)\n",
    "\n",
    "# Takes around 10min for 1000 cases\n",
    "%time unigram_sentences_dict = aggregate_clean_sentences(test_corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list (sentences) of lists (words)\n",
    "unigram_sentences = get_sents_from_sentence_dict(unigram_sentences_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = Phraser(train_phrase_model(unigram_sentences, min_count=2))\n",
    "\n",
    "bigram_sentences = []\n",
    "for sentence in unigram_sentences:\n",
    "    bigram_sentences.append(bigram_model[sentence])\n",
    "    \n",
    "bigram_sentence_dict = {}\n",
    "for label, sentences in unigram_sentences_dict.items():\n",
    "    bigram_sentence_dict[label] = list(bigram_model[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model = Phraser(train_phrase_model(bigram_sentences, min_count=1))\n",
    "\n",
    "trigram_sentences = []\n",
    "for sentence in bigram_sentences:\n",
    "    trigram_sentences.append(trigram_model[sentence])\n",
    "    \n",
    "trigram_sentence_dict = {}\n",
    "for label, sentences in bigram_sentence_dict.items():\n",
    "    trigram_sentence_dict[label] = list(trigram_model[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:  # for debugging\n",
    "    print_label_sent_dict(trigram_sentence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## Create data frame of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from robot_judge.nlp.ngrams import create_df_from_label_sent_dict, get_most_common_words\n",
    "from robot_judge.nlp.ngrams import get_labels_without_year\n",
    "from robot_judge.nlp.ngrams import get_target_values\n",
    "\n",
    "# Index is case label, columns are words, entries are counts\n",
    "feat_df = create_df_from_label_sent_dict(trigram_sentence_dict)\n",
    "target_labels = get_labels_without_year(feat_df)\n",
    "target_values = get_target_values(target_labels)\n",
    "feat_df.insert(0, '__case_reversed__', target_values)\n",
    "feat_df.fillna(0.0, inplace=True)\n",
    "\n",
    "most_common_words = get_most_common_words(trigram_sentence_dict, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = feat_df[most_common_words + ['__case_reversed__']]\n",
    "\n",
    "y = feat_df['__case_reversed__']\n",
    "\n",
    "X = feat_df.loc[:, feat_df.columns != '__case_reversed__']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## Create training / test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('std_scaler', StandardScaler()),\n",
    "                     ('log_regr', LogisticRegression())])\n",
    "\n",
    "param_grid = {'log_regr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_grid=param_grid, cv=2, refit=True)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification report:\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "## GridsearchCV and ROC / AUC evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from scikitplot.metrics import plot_roc\n",
    "from robot_judge.ml import transform_to_text_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_labels = transform_to_text_label(y_test, 'not_reversed', 'reversed')\n",
    "y_pred_labels = transform_to_text_label(y_pred, 'not_reversed', 'reversed')\n",
    "\n",
    "_ = plot_confusion_matrix(y_true_labels, y_pred_labels, figsize=(8, 8), \n",
    "                          title=\"Confusion Matrix for held out test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = clf.predict_proba(X_test)\n",
    "_ = plot_roc(y_true_labels, y_proba, title='ROC Curves for held out test data', figsize=(8, 8), \n",
    "             plot_macro=False, plot_micro=False)\n",
    "print('For AUC score, see plot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 \n",
    "## Vader compound sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "def label_case_dict_to_sentence_list(label_case_dict):\n",
    "    all_sentences = []\n",
    "    for case_text in sampled_cases.values():\n",
    "        for sentence in spacy_nlp(case_text).sents:\n",
    "            all_sentences.append(sentence.text)\n",
    "    return all_sentences\n",
    "\n",
    "all_sentences = label_case_dict_to_sentence_list(sampled_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment_scores(sentences):\n",
    "    sentence_scores = []\n",
    "    for sentence in sentences:\n",
    "        pol_score = sid.polarity_scores(sentence)\n",
    "        \n",
    "        score_dict = {key: value for key, value in pol_score.items()}\n",
    "        score_dict['text'] = sentence\n",
    "        \n",
    "        sentence_scores.append(score_dict)\n",
    "    return sentence_scores\n",
    "\n",
    "sentence_scores = calculate_sentiment_scores(all_sentences[:1000])\n",
    "\n",
    "sent_df = pd.DataFrame(sentence_scores).sort_values(by='pos', ascending=False)\n",
    "print('Most POSITIVE sentences:')\n",
    "for idx, sent in enumerate(sent_df['text'][:10]):\n",
    "    print('\\t', idx, sent)\n",
    "\n",
    "sent_df = pd.DataFrame(sentence_scores).sort_values(by='neg', ascending=False)  \n",
    "print('Most NEGATIVE sentences:')\n",
    "for idx, sent in enumerate(sent_df['text'][:10]):\n",
    "    print('\\t', idx, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8\n",
    "## tf-idf vectorizer and sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from robot_judge.utils.data_structs import sort_coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(all_sentences[:100])\n",
    "X_tfidf_sparse = sparse.csr_matrix(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the ten highest cosine similarities\n",
    "cos_sim = sparse.coo_matrix(sparse.tril(cosine_similarity(X_tfidf_sparse, dense_output=False)))\n",
    "cos_sim = [sim for sim in sort_coo_matrix(cos_sim)]\n",
    "cos_sim = [sim for sim in cos_sim if sim[0] != sim[1]]\n",
    "cos_sim = [sim for sim in cos_sim if sim[2] < 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sentence with high cos similarity\n",
    "\n",
    "for idx_1, idx_2, score in cos_sim:  \n",
    "    print('Cos sim score: {}\\n'.format(score))\n",
    "    print(all_sentences[idx_1])\n",
    "    print(all_sentences[idx_2])\n",
    "    print(20 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9\n",
    "## K-meayns clustering of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_texts(tfidf_model, texts, clusters=3):\n",
    "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=0.1,\n",
    "                                 lowercase=True)\n",
    " \n",
    "    tfidf_model = vectorizer.fit_transform(texts)\n",
    "    km_model = KMeans(n_clusters=clusters)\n",
    "    km_model.fit(tfidf_model)\n",
    " \n",
    "    clustering = collections.defaultdict(list)\n",
    " \n",
    "    for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    " \n",
    "    return clustering\n",
    "\n",
    "\n",
    "clusters = cluster_texts(X_tfidf, all_sentences[:100], 7)\n",
    "\n",
    "def print_sampled_texts_from_clusters(clusters, sentences, n_sample_per_cluster):\n",
    "    for cluster_id, text_indices in clusters.items():\n",
    "        \n",
    "        print('Cluster number {}:'.format(cluster_id))\n",
    "        \n",
    "        sample_indices = random.sample(text_indices, n_sample_per_cluster)\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            print('\\t', sentences[idx])\n",
    "    \n",
    "print_sampled_texts_from_clusters(clusters, all_sentences[:100], 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python skills_ml",
   "language": "python",
   "name": "skills_ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
